{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vehicle Detection and Tracking Project**\n",
    "\n",
    "*Extending Advances Lane Finding Project @ https://github.com/Matz89/CarND-Advanced-Lane-Lines *\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images and train a classifier Linear SVM classifier\n",
    "\n",
    "* Optionally, you can also apply a color transform and append binned color features, as well as histograms of color, to your HOG feature vector.\n",
    "\n",
    "* Note: for those first two steps don't forget to normalize your features and randomize a selection for training and testing.\n",
    "\n",
    "* Implement a sliding-window technique and use your trained classifier to search for vehicles in images.\n",
    "\n",
    "* Run your pipeline on a video stream (start with the test_video.mp4 and later implement on full project_video.mp4) and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles.\n",
    "\n",
    "* Estimate a bounding box for vehicles detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read and display all Test Images**  \n",
    "\n",
    "--using cv2.imread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Test Road Images*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#image reader\n",
    "test_img_path = \".\\\\test_images\\\\\"\n",
    "test_img_filename = '*'\n",
    "test_images = glob.glob(test_img_path + test_img_filename)\n",
    "\n",
    "#images contained here\n",
    "img_shp = cv2.imread(test_images[0]).shape\n",
    "test_imgs = np.empty((len(test_images), img_shp[0], img_shp[1], img_shp[2]), dtype=\"uint8\")\n",
    "\n",
    "#Display all images with matplotlib\n",
    "figx = 2\n",
    "figy = 4\n",
    "#f, axarr = plt.subplots(figx,figy, figsize=(20,10))\n",
    "y = 0\n",
    "#for i, img in enumerate(test_images):\n",
    "    #this_img = cv2.imread(img)\n",
    "    #this_img_resized = cv2.resize(this_img,(img_shp[1], img_shp[0]))\n",
    "    #test_imgs[i] = this_img_resized\n",
    "    #x = i % figx\n",
    "    #axarr[x,y].imshow(cv2.cvtColor(this_img, cv2.COLOR_BGR2RGB))\n",
    "    #axarr[x,y].set_title(img.split('\\\\')[-1])\n",
    "\n",
    "    #y = y+1 if x >= figx-1 else y\n",
    "\n",
    "#Camera image details\n",
    "img_w = img_shp[1]\n",
    "img_h = img_shp[0]\n",
    "img_ch = img_shp[2]\n",
    "num_imgs = len(test_imgs)\n",
    "\n",
    "print(\"Number of Images: {0}\\nImage Width: {1}\\nImage height: {2}\\nColor Channels: {3}\".format(num_imgs, img_w, img_h, img_ch))\n",
    "  \n",
    "#display images\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Camera Calibration Images*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image reader\n",
    "camera_img_path = \".\\\\camera_cal\\\\\"\n",
    "camera_img_filename = '*'\n",
    "camera_images = glob.glob(camera_img_path + camera_img_filename)\n",
    "\n",
    "#images contained here - use shape of first image - fixed sized for greater performance\n",
    "img_shp = cv2.imread(camera_images[0]).shape\n",
    "camera_imgs = np.empty((len(camera_images), img_shp[0], img_shp[1], img_shp[2]), dtype=\"uint8\")\n",
    "\n",
    "#Display all images with matplotlib\n",
    "figx = 5\n",
    "figy = 4\n",
    "#f, axarr = plt.subplots(figx,figy, figsize=(20,10))\n",
    "y = 0\n",
    "#for i, img in enumerate(camera_images):\n",
    "    #this_img = cv2.imread(img)\n",
    "    #this_img_resized = cv2.resize(this_img,(img_shp[1], img_shp[0]))\n",
    "    #camera_imgs[i] = np.uint8(this_img_resized)\n",
    "    #x = i % figx\n",
    "    #axarr[x,y].imshow(cv2.cvtColor(this_img, cv2.COLOR_BGR2RGB))\n",
    "    #axarr[x,y].set_title(img.split('\\\\')[-1])\n",
    "\n",
    "    #y = y+1 if x >= figx-1 else y\n",
    "\n",
    "#Camera image details\n",
    "img_w = img_shp[1]\n",
    "img_h = img_shp[0]\n",
    "img_ch = img_shp[2]\n",
    "num_imgs = len(camera_imgs)\n",
    "\n",
    "print(\"Number of Images: {0}\\nImage Width: {1}\\nImage height: {2}\\nColor Channels: {3}\".format(num_imgs, img_w, img_h, img_ch))\n",
    "    \n",
    "#display images    \n",
    "#plt.tight_layout()\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Camera Calibration Functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For use with chessboard calibration only!\n",
    "#return retval, undist, cornersImg\n",
    "def calibrate_camera(calibration_imgs, nx, ny):\n",
    "    objp = np.zeros((nx*ny, 3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:nx, 0:ny].T.reshape(-1, 2) #object x,y coordinates\n",
    "\n",
    "    objpoints = []\n",
    "    imgpoints = []\n",
    "    \n",
    "    for i, src_img in enumerate(camera_imgs):\n",
    "        img = np.copy(src_img)\n",
    "        \n",
    "        #We assume BGR format\n",
    "        #Convert to Grayscale\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        #Find chessboard corners\n",
    "        ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "\n",
    "        if ret == True:\n",
    "            #Draw and display the corners\n",
    "            cornersImg = cv2.drawChessboardCorners(img, (nx, ny), corners, ret)\n",
    "\n",
    "            objpoints.append(objp)\n",
    "            imgpoints.append(corners)\n",
    "\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)\n",
    "\n",
    "    return mtx, dist\n",
    "\n",
    "    \n",
    "#Example/Test  \n",
    "mtx, dist = calibrate_camera(camera_imgs, 9, 6)\n",
    "\n",
    "#for img in camera_imgs:\n",
    "    \n",
    "    #timg = np.copy(img)\n",
    "    #undist = cv2.undistort(timg, mtx, dist, None, mtx)\n",
    "\n",
    "    #f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    #f.tight_layout()\n",
    "    #ax1.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    #ax1.set_title('Original Image', fontsize=50)\n",
    "    #ax2.imshow(cv2.cvtColor(undist, cv2.COLOR_BGR2RGB)) \n",
    "    #ax2.set_title(\"Undistorted Image\", fontsize=50)\n",
    "    #plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Colour Transforms and Gradients*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Sobel and HLS thresholding to find edges in image\n",
    "def findSobelxHLSThresh(src_img, s_thresh=(170,255), sx_thresh=(20,100), asColour=False):\n",
    "    img = np.copy(src_img)\n",
    "    \n",
    "    #Covert to HLS color space and separate the channels\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_BGR2HLS)\n",
    "    l_ch = hls[:,:,1]\n",
    "    s_ch = hls[:,:,2]\n",
    "    \n",
    "    #Sobel x\n",
    "    sobelx = cv2.Sobel(l_ch, cv2.CV_64F, 1, 0) #Take the derivative in x\n",
    "    abs_sobelx = np.absolute(sobelx) #Absolute x derivative to accentuate lines away from horizontal\n",
    "    scaled_sobel = np.uint8(255*abs_sobelx/np.max(abs_sobelx))\n",
    "    \n",
    "    #Threshold x gradient\n",
    "    sxbinary = np.zeros_like(scaled_sobel)\n",
    "    sxbinary[(scaled_sobel >= sx_thresh[0]) & (scaled_sobel <= sx_thresh[1])] = 1\n",
    "    \n",
    "    \n",
    "    #Threshold color channel\n",
    "    s_binary = np.zeros_like(s_ch)\n",
    "    s_binary[(s_ch >= s_thresh[0]) & (s_ch <= s_thresh[1])] = 1\n",
    "  \n",
    "    #Stack channels; Green = sobelx, Blue = Saturation\n",
    "    if asColour:\n",
    "        color_binary = np.dstack((np.zeros_like(sxbinary), sxbinary, s_binary)) * 255\n",
    "    else:\n",
    "        combined = np.add(sxbinary, s_binary)\n",
    "        color_binary = np.dstack((combined, combined, combined))\n",
    "        color_binary[color_binary > 0] = 255\n",
    "    \n",
    "\n",
    "    return color_binary\n",
    "    \n",
    "        \n",
    "\n",
    "#Example/Test\n",
    "#for i, img in enumerate(test_imgs):\n",
    "\n",
    "    #edgeResult = findSobelxHLSThresh(img, asColour = True)\n",
    "    #f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    #f.tight_layout()\n",
    "\n",
    "    #ax1.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    #ax1.set_title('Original Image', fontsize=40)\n",
    "\n",
    "    #ax2.imshow(edgeResult)\n",
    "    #ax2.set_title('Colour Result', fontsize=40)\n",
    "    #plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Perspective Transform*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defined Region coordinates\n",
    "TOP_LEFT_XY = [.44,.65]\n",
    "TOP_RIGHT_XY = [.56,.65]\n",
    "BOTTOM_LEFT_XY = [.20,.92]\n",
    "BOTTOM_RIGHT_XY = [.80,.92]\n",
    "\n",
    "#overlays image with polygon defined by vertices\n",
    "def view_region_of_interest(img, vertices):\n",
    "    r_img = np.copy(img)\n",
    "    vertices = vertices[0]\n",
    "    prev = vertices[-1]\n",
    "\n",
    "    for v in vertices:\n",
    "        #creating a polygon defined by \"vertices\" with the fill color Blue   \n",
    "        cv2.line(r_img,(prev[0], prev[1]),(v[0],v[1]),(255,0,0),5)\n",
    "        prev = v\n",
    "\n",
    "    return r_img\n",
    "\n",
    "\n",
    "#Example/Test\n",
    "tImage = test_imgs[0]\n",
    "\n",
    "#Define Region of Interest (x,y)\n",
    "v1 = np.multiply(np.flip(tImage.shape[:2], axis=0), BOTTOM_LEFT_XY)  #Bottom Left\n",
    "v2 = np.multiply(np.flip(tImage.shape[:2], axis=0), TOP_LEFT_XY)     #Top Left \n",
    "v3 = np.multiply(np.flip(tImage.shape[:2], axis=0), TOP_RIGHT_XY)    #Top Right\n",
    "v4 = np.multiply(np.flip(tImage.shape[:2], axis=0), BOTTOM_RIGHT_XY) #Bottom Right\n",
    "\n",
    "\n",
    "verts = np.array([v1,v2,v3,v4], dtype=np.int32)\n",
    "\n",
    "region_image = view_region_of_interest(tImage, [verts])\n",
    "\n",
    "#f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "#f.tight_layout()\n",
    "\n",
    "#ax1.imshow(cv2.cvtColor(tImage, cv2.COLOR_BGR2RGB))\n",
    "#ax1.set_title('Original Image', fontsize=40)\n",
    "\n",
    "#ax2.imshow(cv2.cvtColor(region_image, cv2.COLOR_BGR2RGB))\n",
    "#ax2.set_title('Region Result', fontsize=40)\n",
    "#plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Offsets for perspective transform\n",
    "w_offset = 300\n",
    "h_offset = 50\n",
    "\n",
    "def topDownPerspective(img, src_pts, dst_pts):\n",
    "    pImg = np.copy(img)\n",
    "    img_size = (pImg.shape[1], pImg.shape[0])\n",
    "    \n",
    "    M = cv2.getPerspectiveTransform(src_pts, dst_pts)\n",
    "\n",
    "    warped = cv2.warpPerspective(pImg, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    return warped, M\n",
    "\n",
    "#Example/Test\n",
    "for i, img in enumerate(test_imgs):\n",
    "    \n",
    "    shpy = img.shape[0]\n",
    "    shpx = img.shape[1]\n",
    "    img_size = (shpx, shpy)\n",
    "    \n",
    "    src_pts = np.float32(verts)\n",
    "    \n",
    "    dst_tl = [w_offset,h_offset]\n",
    "    dst_tr = [shpx-w_offset,h_offset]\n",
    "    dst_bl = [w_offset,shpy - h_offset]\n",
    "    dst_br = [shpx-w_offset,shpy-h_offset]\n",
    "    \n",
    "    dst_pts = np.float32([dst_bl, dst_tl, dst_tr, dst_br])\n",
    "    undist_img = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    edge_img = findSobelxHLSThresh(undist_img, asColour=False)\n",
    "    result_img, result_M = topDownPerspective(edge_img,src_pts, dst_pts )\n",
    "\n",
    "    #f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    #f.tight_layout()\n",
    "\n",
    "    #ax1.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    #ax1.set_title('Original Image', fontsize=40)\n",
    "\n",
    "    #ax2.imshow(result_img, cmap='gray')\n",
    "    #ax2.set_title('TopDown Result', fontsize=40)\n",
    "    #plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lane Finding*\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identifyLanes(src_img, nwindows=9, margin=100, minpix=50):\n",
    "    #nwindowsl Choose the number of sliding windows\n",
    "    #margin; Set the width of the windows +/- margin\n",
    "    #minpix; Set the minimum number of pixels found to recenter window\n",
    "    \n",
    "    #Prep copy of image\n",
    "    img = np.copy(src_img)\n",
    "    img = np.sum(img, axis=-1)\n",
    "    img[img > 0] = 255\n",
    "    \n",
    "    #collection of rectangle tuples\n",
    "    rectangles = []\n",
    "    \n",
    "    # Take a histogram of the bottom half of the image\n",
    "    histogram = np.sum(img[img.shape[0]//2:,:], axis=0)\n",
    "    \n",
    "    # Create an output image to draw on and  visualize the result\n",
    "    out_img = np.dstack((img,img,img))\n",
    "    out_img = out_img.astype(\"uint8\")\n",
    "    \n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0]//2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "    \n",
    "    # Set height of windows\n",
    "    window_height = np.int(img.shape[0]//nwindows)\n",
    "    \n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = img.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    \n",
    "    # Current positions to be updated for each window\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "    \n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "    \n",
    "    # Step through the windows one by one\n",
    "    for window in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = img.shape[0] - (window+1)*window_height\n",
    "        win_y_high = img.shape[0] - window*window_height\n",
    "        win_xleft_low = leftx_current - margin\n",
    "        win_xleft_high = leftx_current + margin\n",
    "        win_xright_low = rightx_current - margin\n",
    "        win_xright_high = rightx_current + margin\n",
    "        \n",
    "        #Pack away rectangle points to show areas of interest - can be drawn later\n",
    "        rectangles.append(((win_xleft_low,win_y_low),(win_xleft_high,win_y_high)))\n",
    "        rectangles.append(((win_xright_low,win_y_low),(win_xright_high,win_y_high)))\n",
    "\n",
    "        # Identify the nonzero pixels in x and y within the window\n",
    "        good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "        (nonzerox >= win_xleft_low) &  (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "        good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "        (nonzerox >= win_xright_low) &  (nonzerox < win_xright_high)).nonzero()[0]\n",
    "        \n",
    "        # Append these indices to the lists\n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "        \n",
    "        # If you found > minpix pixels, recenter next window on their mean position\n",
    "        if len(good_left_inds) > minpix:\n",
    "            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "        if len(good_right_inds) > minpix:        \n",
    "            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "    \n",
    "    # Concatenate the arrays of indices\n",
    "    left_lane_inds = np.concatenate(left_lane_inds)\n",
    "    right_lane_inds = np.concatenate(right_lane_inds)\n",
    "    \n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds] \n",
    "    \n",
    "    # Fit a second order polynomial to each\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "    \n",
    "    #Colouring lanes left=Red, right=Blue\n",
    "    out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "    out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "    \n",
    "    #Generating x and y values for plotting\n",
    "    ploty = np.linspace(0, out_img.shape[0]-1, out_img.shape[0] )\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    \n",
    "    return out_img, left_fit, right_fit, rectangles, (left_fitx, right_fitx, ploty)\n",
    "    \n",
    "    \n",
    "    \n",
    "#Example/Test\n",
    "for i, img in enumerate(test_imgs):\n",
    "    \n",
    "    shpy = img.shape[0]\n",
    "    shpx = img.shape[1]\n",
    "    img_size = (shpx, shpy)\n",
    "    \n",
    "    src_pts = np.float32(verts)\n",
    "    \n",
    "    dst_tl = [w_offset,h_offset]\n",
    "    dst_tr = [shpx-w_offset,h_offset]\n",
    "    dst_bl = [w_offset,shpy - h_offset]\n",
    "    dst_br = [shpx-w_offset,shpy-h_offset]\n",
    "    \n",
    "    dst_pts = np.float32([dst_bl, dst_tl, dst_tr, dst_br])\n",
    "    undist_img = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    edge_img = findSobelxHLSThresh(undist_img, asColour=False)\n",
    "    topdown_img, topdown_M = topDownPerspective(edge_img,src_pts, dst_pts )\n",
    "    result_img, left_fit, right_fit, rects, img_plots = identifyLanes(topdown_img)\n",
    "    \n",
    "    ploty = img_plots[2]\n",
    "    left_fitx = img_plots[0]\n",
    "    right_fitx = img_plots[1]\n",
    "    \n",
    "    #Draw Rectangles for output image\n",
    "    for pts in rects:\n",
    "        cv2.rectangle(result_img,pts[0],pts[1],(0,255,0), 2) \n",
    "    \n",
    "    #f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    #f.tight_layout()\n",
    "\n",
    "    #ax1.imshow(topdown_img, cmap='gray')\n",
    "    #ax1.set_title('TopDown View', fontsize=40)\n",
    "    \n",
    "    #ax2.imshow(result_img)\n",
    "    #ax2.set_title('Identified Lanes', fontsize=40)\n",
    "    #ax2.plot(left_fitx, ploty, color='yellow')\n",
    "    #ax2.plot(right_fitx, ploty, color='yellow')\n",
    "    #plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identifyLanesNext(src_img, left_fit, right_fit, margin=100):\n",
    "    #If providing left_fit/right_fit polynomials (ie after finding the lanes in the previous frame with identifyLanes)\n",
    "    #This will find new points within the margin of the left_fit/right_fit lines\n",
    "    out_img = np.copy(src_img)\n",
    "    nonzero = out_img.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    \n",
    "    left_lane_inds = ((nonzerox > (left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + \n",
    "    left_fit[2] - margin)) & (nonzerox < (left_fit[0]*(nonzeroy**2) + \n",
    "    left_fit[1]*nonzeroy + left_fit[2] + margin))) \n",
    "\n",
    "    right_lane_inds = ((nonzerox > (right_fit[0]*(nonzeroy**2) + right_fit[1]*nonzeroy + \n",
    "    right_fit[2] - margin)) & (nonzerox < (right_fit[0]*(nonzeroy**2) + \n",
    "    right_fit[1]*nonzeroy + right_fit[2] + margin)))  \n",
    "    \n",
    "    # Again, extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "    # Fit a second order polynomial to each\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "    \n",
    "    # Generate x and y values for plotting\n",
    "    ploty = np.linspace(0, out_img.shape[0]-1, out_img.shape[0] )\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    \n",
    "    #Colouring lanes left=Red, right=Blue\n",
    "    out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "    out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "    \n",
    "    return out_img, left_fit, right_fit, (left_fitx, right_fitx, ploty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Measuring Curvature*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findCurvatureOfLane(ploty, leftx, rightx):\n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "\n",
    "    # Define y-value where we want radius of curvature\n",
    "    # I'll choose the maximum y-value, corresponding to the bottom of the image\n",
    "    y_eval = np.max(ploty)\n",
    "    \n",
    "    # Fit new polynomials to x,y in world space\n",
    "    left_fit_cr = np.polyfit(ploty*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "    right_fit_cr = np.polyfit(ploty*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "    \n",
    "    # Calculate the new radii of curvature\n",
    "    left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "    right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "    \n",
    "    return left_curverad, right_curverad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDevFromCenter(ploty, leftfit, rightfit, car_pos_x, img_h):\n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "\n",
    "    left_x_val = leftfit[0]*img_h**2 + leftfit[1]*img_h + leftfit[2]\n",
    "    right_x_val = rightfit[0]*img_h**2 + rightfit[1]*img_h + rightfit[2]\n",
    "    \n",
    "    #Lane Center position\n",
    "    lane_center_x = (left_x_val + right_x_val) / 2\n",
    "    car_deviation = (car_pos_x - lane_center_x) * xm_per_pix\n",
    "    \n",
    "    return car_deviation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vehicle Detection and Tracking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reading in Vehicle and Non-Vehicle Images for Training/Testing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image reader\n",
    "training_img_path = \".\\\\training\\\\\"\n",
    "non_vehicle_img_filenames = 'non-vehicles\\\\*\\\\*.png'\n",
    "vehicle_img_filenames = 'vehicles\\\\*\\\\*.png'\n",
    "non_vehicle_images = glob.glob(training_img_path + non_vehicle_img_filenames)\n",
    "vehicle_images = glob.glob(training_img_path + vehicle_img_filenames)\n",
    "\n",
    "#images contained here\n",
    "img_shp = cv2.imread(non_vehicle_images[0]).shape\n",
    "non_vehicle_imgs = np.empty((len(non_vehicle_images), img_shp[0], img_shp[1], img_shp[2]), dtype=\"uint8\")\n",
    "\n",
    "#Display random images with matplotlib\n",
    "figx = 2\n",
    "figy = 4\n",
    "f, axarr = plt.subplots(figx,figy, figsize=(20,10))\n",
    "y = 0\n",
    "for i, veh_img in enumerate(vehicle_images):\n",
    "    this_veh_img = cv2.imread(veh_img)\n",
    "    this_non_veh_img = cv2.imread(non_vehicle_images[i])\n",
    "    x = i % figx\n",
    "    axarr[0,y].imshow(cv2.cvtColor(this_veh_img, cv2.COLOR_BGR2RGB))\n",
    "    axarr[0,y].set_title(\"Vehicle Image\")\n",
    "    axarr[1,y].imshow(cv2.cvtColor(this_non_veh_img, cv2.COLOR_BGR2RGB))\n",
    "    axarr[1,y].set_title(\"Non-Vehicle Image\")\n",
    "\n",
    "    y = y+1 if x >= figx-1 else y\n",
    "    if y>=figy:\n",
    "        break\n",
    "\n",
    "#Training image details\n",
    "num_veh_imgs = len(vehicle_images)\n",
    "num_nonveh_imgs = len(non_vehicle_images)\n",
    "\n",
    "print(\"Number of Vehicle Images: {0}\\nNumber of Non-Vehicle Images: {1}\".format(num_veh_imgs, num_nonveh_imgs))\n",
    "  \n",
    "#display images\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Extracting Histogram of Oriented Gradients (HOG) *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "\n",
    "def get_hog_features(img, orient=9, pix_per_cell=8, cell_per_block=2, vis=True, feature_vec=True):\n",
    "                         \n",
    "    return_list = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block),\n",
    "                                  block_norm= 'L2-Hys', transform_sqrt=False, \n",
    "                                  visualise= vis, feature_vector= feature_vec)\n",
    "    \n",
    "    # name returns explicitly\n",
    "    hog_features = return_list[0]\n",
    "    if vis:\n",
    "        hog_image = return_list[1]\n",
    "        return hog_features, hog_image\n",
    "    else:\n",
    "        return hog_features\n",
    "    \n",
    "#Example/Test\n",
    "#Let random index for vehicle image/Non-Vehicle image\n",
    "v_ind = np.random.randint(0, num_veh_imgs)\n",
    "nv_ind = np.random.randint(0, num_nonveh_imgs)\n",
    "\n",
    "#Read Images\n",
    "v_img = cv2.imread(vehicle_images[v_ind])\n",
    "nv_img = cv2.imread(non_vehicle_images[nv_ind])\n",
    "\n",
    "#Get Grayscale of Images\n",
    "v_gray = cv2.cvtColor(v_img, cv2.COLOR_BGR2GRAY)\n",
    "nv_gray = cv2.cvtColor(nv_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Call our function with vis=True to see an image output\n",
    "v_features, v_hog_image = get_hog_features(v_gray, orient= 9, \n",
    "                        pix_per_cell= 8, cell_per_block= 2, \n",
    "                        vis=True, feature_vec=False)\n",
    "\n",
    "nv_features, nv_hog_image = get_hog_features(nv_gray, orient= 9, \n",
    "                        pix_per_cell= 8, cell_per_block= 2, \n",
    "                        vis=True, feature_vec=False)\n",
    "\n",
    "f, axarr = plt.subplots(2,2, figsize=(20,10))\n",
    "axarr[0,0].imshow(v_gray, cmap='gray')\n",
    "axarr[0,0].set_title(\"Vehicle Image\")\n",
    "axarr[0,1].imshow(nv_gray, cmap='gray')\n",
    "axarr[0,1].set_title(\"Non-Vehicle Image\")\n",
    "axarr[1,0].imshow(v_hog_image, cmap='gray')\n",
    "axarr[1,0].set_title(\"HOG Visualization (Vehicle)\")\n",
    "axarr[1,1].imshow(nv_hog_image, cmap='gray')\n",
    "axarr[1,1].set_title(\"HOG Visualization (Non-Vehicle)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Extracting Colour Histogram Features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_hist(img, nbins=32, bins_range=(0, 256)):\n",
    "    \n",
    "    #Separate colour channels and find histograms\n",
    "    channel1_hist = np.histogram(img[:,:,0], bins=nbins, range=bins_range)\n",
    "    channel2_hist = np.histogram(img[:,:,1], bins=nbins, range=bins_range)\n",
    "    channel3_hist = np.histogram(img[:,:,2], bins=nbins, range=bins_range)\n",
    "    \n",
    "    #Concat into single vector\n",
    "    hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "    \n",
    "    return hist_features\n",
    "\n",
    "#Example/Test\n",
    "#Let random index for vehicle image/Non-Vehicle image\n",
    "v_ind = np.random.randint(0, num_veh_imgs)\n",
    "nv_ind = np.random.randint(0, num_nonveh_imgs)\n",
    "\n",
    "#Read Images\n",
    "v_img = cv2.imread(vehicle_images[v_ind])\n",
    "nv_img = cv2.imread(non_vehicle_images[nv_ind])\n",
    "\n",
    "#Colour Histograms\n",
    "v_hists = color_hist(v_img)\n",
    "nv_hists = color_hist(nv_img)\n",
    "\n",
    "v_rh = v_hists[:32]\n",
    "v_gh = v_hists[32:64]\n",
    "v_bh = v_hists[64:]\n",
    "\n",
    "nv_rh = nv_hists[:32]\n",
    "nv_gh = nv_hists[32:64]\n",
    "nv_bh = nv_hists[64:]\n",
    "\n",
    "bin_edges = np.histogram(v_img[:,:,0], bins=32, range=(0,256))[1]\n",
    "bin_centers = (bin_edges[1:]  + bin_edges[0:len(bin_edges)-1])/2\n",
    "\n",
    "fig = plt.figure(figsize=(12,3))\n",
    "plt.subplot(141)\n",
    "plt.imshow(cv2.cvtColor(v_img, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"IMAGE\")\n",
    "plt.subplot(142)\n",
    "plt.bar(bin_centers, v_rh)\n",
    "plt.xlim(0, 256)\n",
    "plt.title('R Histogram')\n",
    "plt.subplot(143)\n",
    "plt.bar(bin_centers, v_gh)\n",
    "plt.xlim(0, 256)\n",
    "plt.title('G Histogram')\n",
    "plt.subplot(144)\n",
    "plt.bar(bin_centers, v_bh)\n",
    "plt.xlim(0, 256)\n",
    "plt.title('B Histogram')\n",
    "fig.tight_layout()\n",
    "\n",
    "fig = plt.figure(figsize=(12,3))\n",
    "plt.subplot(141)\n",
    "plt.imshow(cv2.cvtColor(nv_img, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"IMAGE\")\n",
    "plt.subplot(142)\n",
    "plt.bar(bin_centers, nv_rh)\n",
    "plt.xlim(0, 256)\n",
    "plt.title('R Histogram')\n",
    "plt.subplot(143)\n",
    "plt.bar(bin_centers, nv_gh)\n",
    "plt.xlim(0, 256)\n",
    "plt.title('G Histogram')\n",
    "plt.subplot(144)\n",
    "plt.bar(bin_centers, nv_bh)\n",
    "plt.xlim(0, 256)\n",
    "plt.title('B Histogram')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Extracting Spatial Bin Features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_spatial(img, size=(32, 32)):\n",
    "    #Use Ravel to make image 1D\n",
    "    features = cv2.resize(img, size).ravel() \n",
    "    \n",
    "    return features\n",
    "\n",
    "#Examples/Test\n",
    "#Let random index for vehicle image/Non-Vehicle image\n",
    "v_ind = np.random.randint(0, num_veh_imgs)\n",
    "nv_ind = np.random.randint(0, num_nonveh_imgs)\n",
    "\n",
    "#Read Images\n",
    "v_img = cv2.imread(vehicle_images[v_ind])\n",
    "nv_img = cv2.imread(non_vehicle_images[nv_ind])\n",
    "\n",
    "#extract spatial feature\n",
    "v_spatial = bin_spatial(v_img)\n",
    "nv_spatial = bin_spatial(nv_img)\n",
    "\n",
    "#plot for visual\n",
    "fig = plt.figure(figsize=(20,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(v_spatial)\n",
    "plt.title(\"Vehicle Spatial Features\")\n",
    "plt.subplot(122)\n",
    "plt.plot(nv_spatial)\n",
    "plt.title(\"Non-Vehicle Spatial Features\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Extract features of a multiple images*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def extract_features(img_list, color_space='BGR', spatial_size=(32, 32),\n",
    "                        hist_bins=32, orient=9, \n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0,\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True):   \n",
    "    #Contain all features\n",
    "    features = []\n",
    "    \n",
    "    for file in img_list:\n",
    "        print(file)\n",
    "        #Init empty list to contain image features\n",
    "        img_features = []\n",
    "        \n",
    "        img = cv2.imread(file)\n",
    "\n",
    "        #Colour Conversion\n",
    "        if color_space != 'BGR':\n",
    "            if color_space == 'HSV':\n",
    "                feature_image = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "            elif color_space == 'LUV':\n",
    "                feature_image = cv2.cvtColor(img, cv2.COLOR_BGR2LUV)\n",
    "            elif color_space == 'HLS':\n",
    "                feature_image = cv2.cvtColor(img, cv2.COLOR_BGR2HLS)\n",
    "            elif color_space == 'YUV':\n",
    "                feature_image = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "            elif color_space == 'YCrCb':\n",
    "                feature_image = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n",
    "            elif color_space == 'RGB':\n",
    "                feature_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        else: feature_image = np.copy(img)      \n",
    "\n",
    "        #Spatial Features\n",
    "        if spatial_feat == True:\n",
    "            spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "            img_features.append(spatial_features)\n",
    "\n",
    "        #Histogram Features\n",
    "        if hist_feat == True:\n",
    "            hist_features = color_hist(feature_image, nbins=hist_bins)\n",
    "            img_features.append(hist_features)\n",
    "\n",
    "        #HOG Features\n",
    "        if hog_feat == True:\n",
    "            if hog_channel == 'ALL':\n",
    "                hog_features = []\n",
    "                for channel in range(feature_image.shape[2]):\n",
    "                    hog_features.extend(get_hog_features(feature_image[:,:,channel], \n",
    "                                        orient, pix_per_cell, cell_per_block, \n",
    "                                        vis=False, feature_vec=True))      \n",
    "            else:\n",
    "                hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                            pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "            img_features.append(hog_features)\n",
    "        features.append(np.concatenate(img_features))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Train the model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TUNE-ABLE PARAMETERS\n",
    "color_space = 'HLS'       # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "orient = 9                # HOG orientations\n",
    "pix_per_cell = 8          # HOG pixels per cell\n",
    "cell_per_block = 2        # HOG cells per block\n",
    "hog_channel = 2           # Can be 0, 1, 2, or \"ALL\"\n",
    "spatial_size = (16, 16)   # Spatial binning dimensions\n",
    "hist_bins = 16            # Number of histogram bins\n",
    "spatial_feat = True       # Spatial features on or off\n",
    "hist_feat = False         # Histogram features on or off\n",
    "hog_feat = True           # HOG features on or off\n",
    "y_start_stop = [450, 700] # Min and max in y to search in slide_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#Sample to test home pc\n",
    "sample_size = 500\n",
    "cars = vehicle_images[0:sample_size]\n",
    "notcars = non_vehicle_images[0:sample_size]\n",
    "\n",
    "vehicle_features = extract_features(cars, color_space=color_space, \n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                        orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "\n",
    "non_vehicle_features = extract_features(notcars, color_space=color_space, \n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                        orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "\n",
    "#Stack features\n",
    "X = np.vstack((vehicle_features, non_vehicle_features)).astype(np.float64)\n",
    "\n",
    "#Init labels and stack same order as above\n",
    "y = np.hstack((np.ones(len(vehicle_features)), np.zeros(len(non_vehicle_features))))\n",
    "\n",
    "#Split into training and test sets (random)\n",
    "rand_state = np.random.randint(0, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_state)\n",
    "\n",
    "# Fit a per-column scaler\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "# Apply the scaler to X\n",
    "X_train = X_scaler.transform(X_train)\n",
    "X_test = X_scaler.transform(X_test)\n",
    "\n",
    "print('Using:',orient,'orientations',pix_per_cell,'pixels per cell and', cell_per_block,'cells per block')\n",
    "print('Feature vector length:', len(X_train[0]))\n",
    "# Use a linear SVC \n",
    "svc = LinearSVC()\n",
    "# Check the training time for the SVC\n",
    "t=time.time()\n",
    "svc.fit(X_train, y_train)\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "# Check the score of the SVC\n",
    "print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))\n",
    "# Check the prediction time for a single sample\n",
    "t=time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Window Search*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_windows(img, windows, clf, scaler, color_space='RGB', \n",
    "                    spatial_size=(32, 32), hist_bins=32, \n",
    "                    hist_range=(0, 256), orient=9, \n",
    "                    pix_per_cell=8, cell_per_block=2, \n",
    "                    hog_channel=0, spatial_feat=True, \n",
    "                    hist_feat=True, hog_feat=True):\n",
    "\n",
    "    #Init positive detection window list\n",
    "    on_windows = []\n",
    "\n",
    "    #Find positives\n",
    "    for window in windows:\n",
    "        #3) Extract the test window from original image\n",
    "        test_img = cv2.resize(img[window[0][1]:window[1][1], window[0][0]:window[1][0]], (64, 64))      \n",
    "        #4) Extract features for that window using single_img_features()\n",
    "        features = single_img_features(test_img, color_space=color_space, \n",
    "                            spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                            orient=orient, pix_per_cell=pix_per_cell, \n",
    "                            cell_per_block=cell_per_block, \n",
    "                            hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                            hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "        #5) Scale extracted features to be fed to classifier\n",
    "        test_features = scaler.transform(np.array(features).reshape(1, -1))\n",
    "        #6) Predict using your classifier\n",
    "        prediction = clf.predict(test_features)\n",
    "        #7) If positive (prediction == 1) then save the window\n",
    "        if prediction == 1:\n",
    "            on_windows.append(window)\n",
    "    #8) Return windows for positive detections\n",
    "    return on_windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main Pipeline to Find Lanes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REDEFINE Region Coordinates\n",
    "TOP_LEFT_XY = [.44,.65]\n",
    "TOP_RIGHT_XY = [.56,.65]\n",
    "BOTTOM_LEFT_XY = [.20,.92]\n",
    "BOTTOM_RIGHT_XY = [.80,.92]\n",
    "\n",
    "#Offsets for perspective transform\n",
    "w_offset = 300\n",
    "h_offset = 50\n",
    "\n",
    "#Camera Calibrations\n",
    "mtx, dist = calibrate_camera(camera_imgs, 9, 6)\n",
    "\n",
    "#Main pipeline to find lanes and return augmented image displaying lane information\n",
    "class showLane: \n",
    "    def __init__(self):\n",
    "        self.left_fit = None\n",
    "        self.right_fit = None\n",
    "    def __call__(self, src_img):\n",
    "        #do not edit original image\n",
    "        img = np.copy(src_img)\n",
    "\n",
    "        line_width=50\n",
    "\n",
    "        #Image shape information\n",
    "        shpy = img.shape[0]\n",
    "        shpx = img.shape[1]\n",
    "        img_size = (shpx, shpy)\n",
    "\n",
    "        #Source points for perspective Transform (And region of interest for Lanes)\n",
    "        v1 = np.multiply(np.flip(img.shape[:2], axis=0), BOTTOM_LEFT_XY)  #Bottom Left\n",
    "        v2 = np.multiply(np.flip(img.shape[:2], axis=0), TOP_LEFT_XY)     #Top Left \n",
    "        v3 = np.multiply(np.flip(img.shape[:2], axis=0), TOP_RIGHT_XY)    #Top Right\n",
    "        v4 = np.multiply(np.flip(img.shape[:2], axis=0), BOTTOM_RIGHT_XY) #Bottom Right\n",
    "\n",
    "        src_pts = np.float32(verts)\n",
    "\n",
    "        #Destination points for perspective transform\n",
    "        dst_tl = [w_offset,h_offset]\n",
    "        dst_tr = [shpx-w_offset,h_offset]\n",
    "        dst_bl = [w_offset,shpy - h_offset]\n",
    "        dst_br = [shpx-w_offset,shpy-h_offset]\n",
    "\n",
    "        dst_pts = np.float32([dst_bl, dst_tl, dst_tr, dst_br])\n",
    "\n",
    "        #Undistort image\n",
    "        undist_img = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "\n",
    "        #Push image through threshhold filtering\n",
    "        thresh_img = findSobelxHLSThresh(undist_img, asColour=False)\n",
    "\n",
    "        #Create Top-Down perspective of relevant area of image\n",
    "        topdown_img, topdown_M = topDownPerspective(thresh_img,src_pts, dst_pts )\n",
    "        \n",
    "        if(self.left_fit != None) and (self.right_fit != None):\n",
    "            fit_img, left_fit, right_fit, img_plots = identifyLanesNext(topdown_img, self.left_fit, self.right_fit)\n",
    "            self.left_fit = left_fit\n",
    "            self.right_fit = right_fit\n",
    "        else:\n",
    "            fit_img, left_fit, right_fit, rects, img_plots = identifyLanes(topdown_img)\n",
    "            self.left_fit = left_fit\n",
    "            self.right_fit = right_fit\n",
    "\n",
    "        #separate out img_plots\n",
    "        ploty = img_plots[2]\n",
    "        left_fitx = img_plots[0]\n",
    "        right_fitx = img_plots[1]\n",
    "\n",
    "        #Find curvature of lanes; radius in meters\n",
    "        left_curverad, right_curverad = findCurvatureOfLane(ploty, left_fitx, right_fitx)\n",
    "        \n",
    "        #Find vehicle deviation from center of lane\n",
    "        car_dev = findDevFromCenter(ploty, self.left_fit, self.right_fit, img.shape[1]/2, img.shape[0])\n",
    "\n",
    "        #Create overlay image\n",
    "        out_img_faded = np.zeros_like(fit_img) \n",
    "        out_img = np.zeros_like(fit_img) \n",
    "\n",
    "        #Create Lane Line Visual Info    \n",
    "        #True line width\n",
    "        line_width = int(line_width / 2)\n",
    "\n",
    "        #recast the x and y points into usable format for cv2.fillPoly()\n",
    "        left_line_window1 = np.array([np.transpose(np.vstack([left_fitx-line_width, ploty]))])\n",
    "        left_line_window2 = np.array([np.flipud(np.transpose(np.vstack([left_fitx+line_width, ploty])))])\n",
    "        left_line_pts = np.hstack((left_line_window1, left_line_window2))\n",
    "\n",
    "        right_line_window1 = np.array([np.transpose(np.vstack([right_fitx-line_width, ploty]))])\n",
    "        right_line_window2 = np.array([np.flipud(np.transpose(np.vstack([right_fitx+line_width, ploty])))])\n",
    "        right_line_pts = np.hstack((right_line_window1, right_line_window2))\n",
    "\n",
    "        #Create Lane Region Visual Info\n",
    "        #region_pts = np.concatenate((left_line_pts, np.flipud(right_line_pts)))\n",
    "        region_left = np.array(np.transpose(np.vstack([left_fitx, ploty])))\n",
    "        region_right = np.array(np.transpose(np.vstack([right_fitx, ploty])))\n",
    "        region_pts = np.concatenate((region_left, np.flipud(region_right)))\n",
    "\n",
    "        #Create Lane Curvature Visual Info (Average of both lanes)\n",
    "        curve = np.array(np.transpose(np.vstack([(left_fitx+right_fitx)/2, ploty]))).astype(\"int\")\n",
    "\n",
    "        #Find a point along the line (near the bottom of the image\n",
    "        curve_point = curve[-1]\n",
    "        text_offset = 25;\n",
    "        text_pos = (curve_point[0]+text_offset, curve_point[1]-text_offset)\n",
    "        mid_curve_r = (left_curverad + right_curverad)/2\n",
    "\n",
    "        #Draw Visuals\n",
    "        region_colour = (0,255,0)\n",
    "        left_lane_colour = (255,0,0)\n",
    "        right_lane_colour = (0,0,255)\n",
    "        curve_colour = (155,255,255)\n",
    "        text_colour = (55,55,55)\n",
    "\n",
    "        cv2.fillPoly(out_img_faded, [region_pts.astype('int32')], region_colour)\n",
    "        cv2.fillPoly(out_img_faded, np.int_([left_line_pts]), left_lane_colour)\n",
    "        cv2.fillPoly(out_img_faded, np.int_([right_line_pts]), right_lane_colour)\n",
    "\n",
    "        start_pt = curve[0]\n",
    "        for end_pt in curve[1:]:\n",
    "            cv2.line(out_img, (start_pt[0], start_pt[1]), (end_pt[0], end_pt[1]), text_colour, 5)\n",
    "            start_pt = end_pt\n",
    "\n",
    "        #Return perspective to original view\n",
    "        persp_img_faded, out_M = topDownPerspective(out_img_faded,dst_pts, src_pts )\n",
    "        persp_img, out_M = topDownPerspective(out_img,dst_pts, src_pts )\n",
    "\n",
    "        #Overlay with original image\n",
    "        persp_img = cv2.addWeighted(persp_img_faded, 0.5, persp_img, 1.0, 0.0)\n",
    "        out_img = cv2.addWeighted(persp_img, 1.0, undist_img, 1.0, 0.0)\n",
    "\n",
    "        #Drawn interface information\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(out_img, \"%.2fm (r.)\" % mid_curve_r, text_pos, font, 1.0, curve_colour, 5)\n",
    "        cv2.putText(out_img, \"dev from cntr: %.2fm\" % car_dev, (15, 45), font, 1.0, curve_colour, 5)\n",
    "\n",
    "        return out_img\n",
    "\n",
    "#Example/Test\n",
    "#Image Test\n",
    "t_img = test_imgs[4]\n",
    "\n",
    "tt = showLane()\n",
    "r_img = tt(t_img)\n",
    "\n",
    "\n",
    "#Plot Example Image\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "f.tight_layout()\n",
    "\n",
    "ax1.imshow(cv2.cvtColor(t_img, cv2.COLOR_BGR2RGB))\n",
    "ax1.set_title('TopDown View', fontsize=40)\n",
    "\n",
    "ax2.imshow(cv2.cvtColor(r_img, cv2.COLOR_BGR2RGB))\n",
    "ax2.set_title('Pipeline Test', fontsize=40)\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run pipeline with easy video sample*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#inp_vid = \"./project_video.mp4\"\n",
    "#out_vid = \"./output_videos/out_project_video.mp4\"\n",
    "#clip1 = VideoFileClip(inp_vid)\n",
    "#out_clip = clip1.fl_image(showLane())\n",
    "#out_clip.write_videofile(out_vid, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
